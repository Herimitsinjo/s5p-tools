#!/usr/local/bin/python

from datetime import datetime
import argparse
from os.path import exists
from os import makedirs
import warnings

import xarray as xr
from dask.diagnostics import ProgressBar

from s5p_tools.s5p_tools import geojson_window, convert_to_l3_products, request_copernicus_hub, get_filenames_request, make_country_mask
from s5p_tools.pretty_print import printCyan, printRed, printBold


# Ignore warnings
warnings.filterwarnings("ignore", category=RuntimeWarning)


# -------------------------------------------------------------------
# -------------------------- PARAMS ---------------------------------
# -------------------------------------------------------------------

# Perform checksum verification after each download
CHECKSUM = True


# -------------------------------------------------------------------
# ------------------------------ PATHS ------------------------------
# -------------------------------------------------------------------

# download_directory: directory for L2 products
DOWNLOAD_DIR = 'L2_data'

# export_directory: directory for L3 products
EXPORT_DIR = 'L3_data'

# processed_directory: directory for processed products (aggregated+masked)
PROCESSED_DIR = 'processed'


# -------------------------------------------------------------------
# ------------------------- PARSE ARGUMENTS -------------------------
# -------------------------------------------------------------------

parser = argparse.ArgumentParser(
    description='Request, download and process Sentinel data from Copernicus access hub'
)

# Product type: Used to perform a product based search
# Possible values are
#   L2__O3____
#   L2__NO2___
#   L2__SO2___
#   L2__CO____
#   L2__CH4___
#   L2__HCHO__
#   L2__AER_AI
#   L2__CLOUD_
parser.add_argument('product', help='Product type', type=str)


# Date: Used to perform a time interval search
# The general form to be used is:
#       date=(< timestamp>, <timestamp>)
# where < timestamp > can be expressed in one of the following formats:
#   yyyyMMdd
#   yyyy-MM-ddThh:mm:ssZ
#   yyyy-MM-ddThh:mm:ss.SSSZ(ISO8601 format)
#   NOW
#   NOW-<n>MINUTE(S)
#   NOW-<n>HOUR(S)
#   NOW-<n>DAY(S)
#   NOW-<n>MONTH(S)
parser.add_argument('--date', help='Date', nargs='+',
                    type=str, default=('NOW-24HOURS', 'NOW'))


# Area of interest: The url of the area of interest (.geojson file) relative to current directory
parser.add_argument(
    '--aoi', help='The url of the area of interest (.geojson file) relative to current directory', type=str)


# Shapefile: The url of the shapefile (.shp) for pixel filtering
parser.add_argument('--shp', help='The url of the shapefile (.shp) relative to current directory for pixel filtering',
                    type=str)


# Platform: Name of the platform
parser.add_argument('--platform', help='Platform name', type=str)


# Harp command: Harp convert command used during import of products
parser.add_argument('--command', help='Harp convert command used during import of products',
                    type=str)


# Unit: Unit conversion
parser.add_argument('--unit', help='Unit conversion',
                    default='molec/m2', type=str)


args = parser.parse_args()


# -------------------------------------------------------------------
# ------------- QUERYING & DOWNLOADING PRODUCTS ---------------------
# -------------------------------------------------------------------

DHUS_USER = 's5pguest'
DHUS_PASSWORD = 's5pguest'
DHUS_URL = 'https://s5phub.copernicus.eu/dhus'

FIX_EXTENSION = True

printCyan('\nRequest products\n')

if args.platform is None:
    _, products = request_copernicus_hub(login=DHUS_USER,
                                         password=DHUS_PASSWORD,
                                         hub=DHUS_URL,
                                         aoi=args.aoi,
                                         date=tuple(args.date),
                                         producttype=args.product,
                                         download_directory='{dir}/{product_dir}'.format(dir=DOWNLOAD_DIR,
                                                                                         product_dir=args.product),
                                         checksum=CHECKSUM,
                                         fix_extension=FIX_EXTENSION)

else:
    _, products = request_copernicus_hub(login=DHUS_USER,
                                         password=DHUS_PASSWORD,
                                         hub=DHUS_URL,
                                         aoi=args.aoi,
                                         date=tuple(args.date),
                                         platformname=args.platform,
                                         producttype=args.product,
                                         download_directory='{dir}/{product_dir}'.format(dir=DOWNLOAD_DIR,
                                                                                         product_dir=args.product),
                                         checksum=CHECKSUM,
                                         fix_extension=FIX_EXTENSION)


L2_files_urls = get_filenames_request(products, '{dir}/{product_dir}'.format(dir=DOWNLOAD_DIR,
                                                                             product_dir=args.product))


if len(L2_files_urls) > 0:

    # -------------------------------------------------------------------
    # ----------------------- PREPROCESS DATA ---------------------------
    # -------------------------------------------------------------------

    printCyan('\nConvert into L3 products\n')

    # harpconvert commands :
    # the source data is filtered to remove any values with QA values less than 50 according to Copernicus specifications
    # + binning data by latitude/longitude

    harp_commands = {

        'L2__NO2___': ('tropospheric_NO2_column_number_density_validity>50;'
                       'tropospheric_NO2_column_number_density>=0;'
                       'NO2_column_number_density>=0;'
                       'stratospheric_NO2_column_number_density>=0;'
                       'NO2_slant_column_number_density>=0;'
                       'derive(tropospheric_NO2_column_number_density [{unit}]);'
                       'derive(stratospheric_NO2_column_number_density [{unit}]);'
                       'derive(NO2_column_number_density [{unit}]);'
                       'derive(NO2_slant_column_number_density [{unit}])').format(unit=args.unit),

        'L2__O3____': ('O3_column_number_density_validity>50;'
                       'O3_column_number_density>=0;'
                       'derive(O3_column_number_density [{unit}])').format(unit=args.unit),

        'L2__SO2___': ('SO2_column_number_density_validity>50;'
                       'SO2_column_number_density>=0;'
                       'SO2_slant_column_number_density>=0;'
                       'O3_column_number_density>=0;'
                       'derive(SO2_column_number_density [{unit}]);'
                       'derive(SO2_slant_column_number_density [{unit}]);'
                       'derive(O3_column_number_density [{unit}])').format(unit=args.unit),

        'L2__HCHO__': ('tropospheric_HCHO_column_number_density_validity>50;'
                       'tropospheric_HCHO_column_number_density>=0;'
                       'HCHO_slant_column_number_density>=0;'
                       'derive(tropospheric_HCHO_column_number_density [{unit}]);'
                       'derive(HCHO_slant_column_number_density [{unit}])').format(unit=args.unit),

        'L2__CO____': ('CO_column_number_density_validity>50;'
                       'H2O_column_number_density>=0;'
                       'derive(CO_column_number_density [{unit}]);'
                       'derive(H2O_column_number_density [{unit}])').format(unit=args.unit),

        'L2__CH4___': ('CH4_column_volume_mixing_ratio_dry_air_validity>50;'
                       'H2O_column_number_density>=0;'
                       'derive(H2O_column_number_density [{unit}]);'
                       'derive(dry_air_column_number_density [{unit}])').format(unit=args.unit),

        'L2__AER_AI': ('absorbing_aerosol_index_validity>50').format(unit=args.unit),

        'L2__CLOUD_': ('cloud_fraction_validity>50;'
                       'cloud_fraction>=0').format(unit=args.unit)

    }

    # Step size for spatial re-gridding (in degrees)
    LON_STEP = 0.01
    LAT_STEP = 0.01

    if args.aoi is None:
        extent = [-180, 180, -90, 90]
    else:
        # extent: compute map extent from aoi
        extent = geojson_window(args.aoi)

    # computes offsets and number of samples
    lat_edge_length = int(abs(extent[3] - extent[2]) / LAT_STEP + 1)
    lat_edge_offset = extent[2]
    lon_edge_length = int(abs(extent[1] - extent[0]) / LON_STEP + 1)
    lon_edge_offset = extent[0]

    if args.command is None:
        conversion_command = harp_commands[args.product]
    else:
        conversion_command = args.command

    pre_commands = ('{conversion_command};exclude(datetime_length);'
                    'bin_spatial({lat_edge_length},{lat_edge_offset},{lat_step},{lon_edge_length},{lon_edge_offset},'
                    '{lon_step});derive(latitude {{latitude}});derive(longitude {{longitude}})').format(
                        lat_edge_length=lat_edge_length,
                        lat_edge_offset=lat_edge_offset,
                        lat_step=LAT_STEP,
                        lon_edge_length=lon_edge_length,
                        lon_edge_offset=lon_edge_offset,
                        lon_step=LON_STEP,
                        conversion_command=conversion_command)

    # perform conversion
    L3_product_url = convert_to_l3_products(L2_files_urls, pre_commands, export_path='{dir}/{product_dir}'.format(dir=EXPORT_DIR,
                                            product_dir=args.product.replace('L2', 'L3')))


    # -------------------------------------------------------------------
    # ------------------ AGGREGRATE PRODUCTS ----------------------------
    # -------------------------------------------------------------------

    printCyan('\nProcess data\n')

    # Avoid lost attributes during conversion
    xr.set_options(keep_attrs=True)

    with ProgressBar():

        # open L2 products
        DS = xr.open_mfdataset([filename.replace('L2', 'L3') for filename in L2_files_urls
                                if exists(filename.replace('L2', 'L3'))], combine='nested', concat_dim='time')

        # create a new coordinate time
        DS = DS.assign_coords(time=DS.datetime_start)
        DS = DS.drop('datetime_start')
        DS = DS.sortby('time')

        # filter pixels
        if args.shp is not None:
            mask = make_country_mask(args.shp, DS.longitude, DS.latitude)

            for column in [column_name for column_name in list(DS.variables)
                           if DS[column_name].dims == ('time', 'latitude', 'longitude')]:
                DS[column] = DS[column].where(mask)


    # -------------------------------------------------------------------
    # ---------------------- EXPORT PRODUCTS ----------------------------
    # -------------------------------------------------------------------

    printCyan('\nExport data...\n')

    start = min([products[uuid]['beginposition'] for uuid in products.keys()])
    end = max([products[uuid]['endposition'] for uuid in products.keys()])

    if not exists('{dir}/processed{product}'.format(dir=PROCESSED_DIR, product=args.product[2:])):
        makedirs('{dir}/processed{product}'.format(dir=PROCESSED_DIR,
                                                   product=args.product[2:]))

    file_export_name = ('{dir}/processed{product}/{product_type}{start_day}-{start_month}-{start_year}__'
                        '{end_day}-{end_month}-{end_year}.nc').format(dir=PROCESSED_DIR,
                                                                      product=args.product[2:],
                                                                      start_day=start.day,
                                                                      start_month=start.month,
                                                                      start_year=start.year,
                                                                      end_day=end.day,
                                                                      end_month=end.month,
                                                                      end_year=end.year,
                                                                      product_type=args.product[4:])

    with ProgressBar():
        DS.to_netcdf(file_export_name)

printCyan('\nDone\n')
