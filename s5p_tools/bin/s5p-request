#!/usr/local/bin/python

from datetime import datetime
import argparse
from os.path import exists
from os import makedirs
import warnings

import xarray as xr
from dask.diagnostics import ProgressBar

from s5p_tools.s5p_tools import geojson_window, convert_to_l3_products, request_copernicus_hub, get_filenames_request, make_country_mask
from s5p_tools.pretty_print import printCyan, printRed, printBold


# Ignore warnings
warnings.filterwarnings("ignore", category=RuntimeWarning)


# -------------------------------------------------------------------
# -------------------------- PARAMS ---------------------------------
# -------------------------------------------------------------------

# Perform checksum verification after each download
CHECKSUM = True


# -------------------------------------------------------------------
# ------------------------- PARSE ARGUMENTS -------------------------
# -------------------------------------------------------------------

parser = argparse.ArgumentParser(
    description='Request, download and process Sentinel data from Copernicus access hub'
)

# Product type: Used to perform a product based search
# Possible values are
#   L2__O3____
#   L2__NO2___
#   L2__SO2___
#   L2__CO____
#   L2__CH4___
#   L2__HCHO__
#   L2__AER_AI
#   L2__CLOUD_
parser.add_argument('product', help='Product type', type=str)


# Date: Used to perform a time interval search
# The general form to be used is:
#       date=(< timestamp>, <timestamp>)
# where < timestamp > can be expressed in one of the following formats:
#   yyyyMMdd
#   yyyy-MM-ddThh:mm:ssZ
#   yyyy-MM-ddThh:mm:ss.SSSZ(ISO8601 format)
#   NOW
#   NOW-<n>MINUTE(S)
#   NOW-<n>HOUR(S)
#   NOW-<n>DAY(S)
#   NOW-<n>MONTH(S)
parser.add_argument('--date', help='Date', nargs='+',
                    type=str, default=('NOW-24HOURS', 'NOW'))


# Area of interest: The url of the area of interest (.geojson file) relative to current directory
parser.add_argument(
    '--aoi', help='The url of the area of interest (.geojson file) relative to current directory', type=str)


# Shapefile: The url of the shapefile (.shp) for pixel filtering
parser.add_argument('--shp', help='The url of the shapefile (.shp) relative to current directory for pixel filtering',
                    type=str)


# Platform: Name of the platform
parser.add_argument('--platform', help='Platform name', type=str)


# Harp command: Harp convert command used during import of products
parser.add_argument('--command', help='Harp convert command used during import of products',
                    type=str)


# Unit: Unit conversion
parser.add_argument('--unit', help='Unit conversion', type=str)


args = parser.parse_args()


# -------------------------------------------------------------------
# ------------------------------ PATHS ------------------------------
# -------------------------------------------------------------------

# download_directory: directory for L2 products
DOWNLOAD_DIR = 'L2_data'

# export_directory: directory for L3 products
EXPORT_DIR = 'L3_data'

# processed_directory: directory for processed products (aggregated+masked)
PROCESSED_DIR = 'processed'


# -------------------------------------------------------------------
# ------------- QUERYING & DOWNLOADING PRODUCTS ---------------------
# -------------------------------------------------------------------

DHUS_USER = 's5pguest'
DHUS_PASSWORD = 's5pguest'
DHUS_URL = 'https://s5phub.copernicus.eu/dhus'

FIX_EXTENSION = True

printCyan('\nRequest products\n')

_, products = request_copernicus_hub(login=DHUS_USER,
                                    password=DHUS_PASSWORD,
                                    hub=DHUS_URL,
                                    aoi=args.aoi,
                                    date=tuple(args.date),
                                    platformname='Sentinel-5 Precursor',
                                    producttype=args.product,
                                    download_directory='{dir}/{product_dir}'.format(dir=DOWNLOAD_DIR,
                                                                                    product_dir=args.product),
                                    checksum=CHECKSUM,
                                    fix_extension=FIX_EXTENSION)


L2_files_urls = get_filenames_request(products, '{dir}/{product_dir}'.format(dir=DOWNLOAD_DIR,
                                                                             product_dir=args.product))


if len(L2_files_urls) > 0:

    # -------------------------------------------------------------------
    # ----------------------- PREPROCESS DATA ---------------------------
    # -------------------------------------------------------------------

    printCyan('\nConvert into L3 products\n')

    # harpconvert commands :
    # the source data is filtered to remove any values with QA values less than 50 according to Copernicus specifications
    # + binning data by latitude/longitude

    harp_filter_commands = {

        'L2__NO2___': ('tropospheric_NO2_column_number_density_validity>50;'
                       'tropospheric_NO2_column_number_density>=0;'
                       'NO2_column_number_density>=0;'
                       'stratospheric_NO2_column_number_density>=0;'
                       'NO2_slant_column_number_density>=0'),

        'L2__O3____': ('O3_column_number_density_validity>50;'
                       'O3_column_number_density>=0'),

        'L2__SO2___': ('SO2_column_number_density_validity>50;'
                       'SO2_column_number_density>=0;'
                       'SO2_slant_column_number_density>=0;'
                       'O3_column_number_density>=0'),

        'L2__HCHO__': ('tropospheric_HCHO_column_number_density_validity>50;'
                       'tropospheric_HCHO_column_number_density>=0;'
                       'HCHO_slant_column_number_density>=0'),

        'L2__CO____': ('CO_column_number_density_validity>50;'
                       'H2O_column_number_density>=0'),

        'L2__CH4___': ('CH4_column_volume_mixing_ratio_dry_air_validity>50;'
                       'H2O_column_number_density>=0'),

        'L2__AER_AI': ('absorbing_aerosol_index_validity>50'),

        'L2__CLOUD_': ('cloud_fraction_validity>50;'
                       'cloud_fraction>=0')

    }

    harp_conversion_commands = {

        'L2__NO2___': ('derive(tropospheric_NO2_column_number_density [{unit}]);'
                       'derive(stratospheric_NO2_column_number_density [{unit}]);'
                       'derive(NO2_column_number_density [{unit}]);'
                       'derive(NO2_slant_column_number_density [{unit}])').format(unit=args.unit),

        'L2__O3____': ('derive(O3_column_number_density [{unit}])').format(unit=args.unit),

        'L2__SO2___': ('derive(SO2_column_number_density [{unit}]);'
                       'derive(SO2_slant_column_number_density [{unit}]);'
                       'derive(O3_column_number_density [{unit}])').format(unit=args.unit),

        'L2__HCHO__': ('derive(tropospheric_HCHO_column_number_density [{unit}]);'
                       'derive(HCHO_slant_column_number_density [{unit}])').format(unit=args.unit),

        'L2__CO____': ('derive(CO_column_number_density [{unit}]);'
                       'derive(H2O_column_number_density [{unit}])').format(unit=args.unit),

        'L2__CH4___': ('derive(H2O_column_number_density [{unit}]);'
                       'derive(dry_air_column_number_density [{unit}])').format(unit=args.unit),

        'L2__AER_AI': '',

        'L2__CLOUD_': ''

    }

    harp_keep_commands = {

        'L2__NO2___': ('keep(NO2_column_number_density,tropospheric_NO2_column_number_density,'
                       'stratospheric_NO2_column_number_density,NO2_slant_column_number_density,'
                       'tropopause_pressure,absorbing_aerosol_index,cloud_fraction)'),

        'L2__O3____': ('keep(O3_column_number_density,O3_column_number_density_amf, O3_slant_column_number_density,'
                       'O3_effective_temperature,cloud_fraction)'),

        'L2__SO2___': ('keep(SO2_column_number_density,SO2_column_number_density_amf, SO2_slant_column_number_density,'
                       'absorbing_aerosol_index,cloud_fraction)'),

        'L2__HCHO__': ('keep(tropospheric_HCHO_column_number_density,'
                       'tropospheric_HCHO_column_number_density_amf,'
                       'HCHO_slant_column_number_density,cloud_fraction'),

        'L2__CO____': ('keep(CO_column_number_density,H2O_column_number_density,cloud_height)'),

        'L2__CH4___': ('keep(CH4_column_volume_mixing_ratio_dry_air, aerosol_height,'
                       'aerosol_optical_depth)'),

        'L2__AER_AI': 'keep(absorbing_aerosol_index)',

        'L2__CLOUD_': 'keep(cloud_fraction,cloud_top_pressure,cloud_top_height, cloud_base_pressure,'
                      'cloud_base_height,cloud_optical_depth,surface_albedo)'

    }

    # Step size for spatial re-gridding (in degrees)
    LON_STEP = 0.01
    LAT_STEP = 0.01

    if args.aoi is None:
        extent = [-180, 180, -90, 90]
    else:
        # extent: compute map extent from aoi
        extent = geojson_window(args.aoi)

    # computes offsets and number of samples
    lat_edge_length = int(abs(extent[3] - extent[2]) / LAT_STEP + 1)
    lat_edge_offset = extent[2]
    lon_edge_length = int(abs(extent[1] - extent[0]) / LON_STEP + 1)
    lon_edge_offset = extent[0]

    if args.command is None:
        if args.unit is None:
            harp_commands = '{filter_command}'.format(
                filter_command=harp_filter_commands[args.product])
        else:
            harp_commands = '{filter_command};{conversion_command}'.format(filter_command=harp_filter_commands[args.product],
                                                                    conversion_command=harp_conversion_commands[args.product])

        pre_commands = ('{harp_commands};exclude(datetime_length);'
                    'bin_spatial({lat_edge_length},{lat_edge_offset},{lat_step},{lon_edge_length},{lon_edge_offset},'
                    '{lon_step});derive(latitude {{latitude}});derive(longitude {{longitude}});{keep_commands}').format(
                        lat_edge_length=lat_edge_length,
                        lat_edge_offset=lat_edge_offset,
                        lat_step=LAT_STEP,
                        lon_edge_length=lon_edge_length,
                        lon_edge_offset=lon_edge_offset,
                        lon_step=LON_STEP,
                        harp_commands=harp_commands,
                        keep_commands=harp_keep_commands[args.product])

    else:
        harp_commands = args.command
        pre_commands = ('exclude(datetime_length);'
                    'bin_spatial({lat_edge_length},{lat_edge_offset},{lat_step},{lon_edge_length},{lon_edge_offset},'
                    '{lon_step});derive(latitude {{latitude}});derive(longitude {{longitude}});{harp_commands}').format(
                        lat_edge_length=lat_edge_length,
                        lat_edge_offset=lat_edge_offset,
                        lat_step=LAT_STEP,
                        lon_edge_length=lon_edge_length,
                        lon_edge_offset=lon_edge_offset,
                        lon_step=LON_STEP,
                        harp_commands=harp_commands)

    # perform conversion
    L3_product_url = convert_to_l3_products(L2_files_urls, pre_commands, export_path='{dir}/{product_dir}'.format(dir=EXPORT_DIR,
                                            product_dir=args.product.replace('L2', 'L3')))

    # -------------------------------------------------------------------
    # ------------------ AGGREGRATE PRODUCTS ----------------------------
    # -------------------------------------------------------------------

    printCyan('\nProcess data\n')

    # Avoid lost attributes during conversion
    xr.set_options(keep_attrs=True)

    # open L2 products
    print('Loading data\n')

    def preprocess(ds):
        ds = ds.assign_coords(time=ds.datetime_start)
        ds = ds.drop('datetime_start')
        return ds

    with ProgressBar():
        DS = xr.open_mfdataset([filename.replace('L2', 'L3') for filename in L2_files_urls
                                if exists(filename.replace('L2', 'L3'))], combine='by_coords',
                                preprocess=lambda ds: preprocess(ds), chunks={'time': 100})

    # filter pixels
    with ProgressBar():
        if args.shp is not None:
            print('\nApplying shapefile\n')
            mask = make_country_mask(args.shp, DS.longitude, DS.latitude)
            for column in [column_name for column_name in list(DS.variables)
                           if DS[column_name].dims == ('time', 'latitude', 'longitude')]:
                DS[column] = DS[column].where(mask)

    # -------------------------------------------------------------------
    # ---------------------- EXPORT PRODUCTS ----------------------------
    # -------------------------------------------------------------------

    printCyan('\nExport dataset\n')

    start = min([products[uuid]['beginposition'] for uuid in products.keys()])
    end = max([products[uuid]['endposition'] for uuid in products.keys()])

    if not exists('{dir}/processed{product}'.format(dir=PROCESSED_DIR, product=args.product[2:])):
        makedirs('{dir}/processed{product}'.format(dir=PROCESSED_DIR,
                                                   product=args.product[2:]))

    file_export_name = ('{dir}/processed{product}/{product_type}{start_day}-{start_month}-{start_year}__'
                        '{end_day}-{end_month}-{end_year}.nc').format(dir=PROCESSED_DIR,
                                                                      product=args.product[2:],
                                                                      start_day=start.day,
                                                                      start_month=start.month,
                                                                      start_year=start.year,
                                                                      end_day=end.day,
                                                                      end_month=end.month,
                                                                      end_year=end.year,
                                                                      product_type=args.product[4:])

    with ProgressBar():
        DS.to_netcdf(file_export_name)

printCyan('\nDone\n')
